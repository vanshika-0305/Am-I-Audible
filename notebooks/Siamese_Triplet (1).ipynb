{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woh9QSmkCvEv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers,models\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chiQRn2mMPHA",
        "outputId": "f9ac95c3-840c-4795-d39f-8b29fe9925ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I15NK6u4FGAN"
      },
      "outputs": [],
      "source": [
        "Data_dir = \"/content/drive/MyDrive/Datasets/vox_indian\"\n",
        "input_shape= (128,128)\n",
        "embedding_size = 256\n",
        "num_samples=500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opbSehSIFTXG"
      },
      "outputs": [],
      "source": [
        "def load_speaker_map(data_dir):\n",
        "    speaker_map = {}\n",
        "    for speaker in os.listdir(data_dir):\n",
        "        speaker_path = os.path.join(data_dir, speaker)\n",
        "        if os.path.isdir(speaker_path):\n",
        "            files = []\n",
        "            for root, _, filenames in os.walk(speaker_path):\n",
        "                files.extend([os.path.join(root, f) for f in filenames])\n",
        "            speaker_map[speaker] = files\n",
        "    return speaker_map\n",
        "\n",
        "Data_dir = \"/content/drive/MyDrive/Datasets/vox_indian\"\n",
        "speaker_map = load_speaker_map(Data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvBAekeVFUQJ",
        "outputId": "0b7aa11b-e464-4fd0-cb71-37519f404814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing audio processing...\n",
            "DEMO OUTPUT: Processed spectrogram shape: (128, 128, 1)\n",
            "Sample values:\n",
            "[0.60858566 0.5791833  0.53700113 0.48924765 0.45931637]\n"
          ]
        }
      ],
      "source": [
        "def pre_process(file_path, target_shape=(128, 128), sr=16000):\n",
        "    \"\"\"Returns mel-spectrogram with shape (128, 128, 1)\"\"\"\n",
        "    y, _ = librosa.load(file_path, sr=sr)\n",
        "    y = librosa.util.fix_length(y, size=3*sr)  # 3 seconds x 16k samplerate=48k\n",
        "    mel = librosa.feature.melspectrogram(y=y, sr=sr,\n",
        "                                       n_mels=target_shape[0],\n",
        "                                       hop_length=512)\n",
        "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
        "    mel_db = (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min() + 1e-6)\n",
        "    mel_db = tf.image.resize(mel_db[..., tf.newaxis], target_shape)\n",
        "\n",
        "    return mel_db.numpy()\n",
        "\n",
        "print(\"\\nTesting audio processing...\")\n",
        "sample_file = list(load_speaker_map(Data_dir).values())[0][0]  # Get first file\n",
        "sample_mel = pre_process(sample_file)\n",
        "print(f\"DEMO OUTPUT: Processed spectrogram shape: {sample_mel.shape}\")\n",
        "print(f\"Sample values:\\n{sample_mel[0, :5, 0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp0_ikwldO1F"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_triplets(speakers, model, num_samples=100):\n",
        "\n",
        "    triplet_paths = []\n",
        "    spk_list = list(speakers.keys())\n",
        "\n",
        "    with tqdm(total=num_samples, desc=\"Creating triplets with hard negatives\") as pbar:\n",
        "        for _ in range(num_samples):\n",
        "            anchor_spk = random.choice(spk_list)\n",
        "            while len(speakers[anchor_spk]) < 2:\n",
        "                anchor_spk = random.choice(spk_list)\n",
        "\n",
        "            # Select anchor and positive from the same speaker\n",
        "            anchor, positive = random.sample(speakers[anchor_spk], 2)\n",
        "            processed_anchor = pre_process(anchor)\n",
        "            print(\"Processed shape:\", processed_anchor.shape)  # Debugging\n",
        "\n",
        "            anchor_embedding = model.predict(np.expand_dims(processed_anchor, axis=0))\n",
        "            # Find the hardest negative (most similar to anchor)\n",
        "            hardest_neg = None\n",
        "            max_similarity = -1\n",
        "\n",
        "            for neg_spk in spk_list:\n",
        "                if neg_spk == anchor_spk:\n",
        "                    continue\n",
        "                for neg_sample in speakers[neg_spk]:\n",
        "                    neg_embedding = model.predict(np.expand_dims(pre_process(neg_sample), axis=0))\n",
        "                    similarity = 1 - cosine(anchor_embedding.flatten(), neg_embedding.flatten())  # Cosine similarity\n",
        "\n",
        "                    if similarity > max_similarity:\n",
        "                        max_similarity = similarity\n",
        "                        hardest_neg = neg_sample\n",
        "\n",
        "            triplet_paths.append((anchor, positive, hardest_neg))\n",
        "            pbar.update(1)\n",
        "\n",
        "    print(\"\\nDEMO OUTPUT: First 3 triplets:\")\n",
        "    for t in triplet_paths[:3]:\n",
        "        print(f\"A: {os.path.basename(t[0])} | P: {os.path.basename(t[1])} | N: {os.path.basename(t[2])}\")\n",
        "\n",
        "    return triplet_paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0VQkiSoGMD8",
        "outputId": "9d0ecdd6-bced-43bf-ae90-dafe6f4f82f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEMO OUTPUT: Input shape: (1, 128, 128, 1)\n",
            "Model output shape: (1, 256)\n"
          ]
        }
      ],
      "source": [
        "def create_encoder():\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(128, 128, 1)),\n",
        "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dense(256)\n",
        "    ])\n",
        "\n",
        "class SiameseModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(SiameseModel, self).__init__()\n",
        "        self.encoder = create_encoder()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        anc = self.encoder(inputs[0])\n",
        "        pos = self.encoder(inputs[1])\n",
        "        neg = self.encoder(inputs[2])\n",
        "\n",
        "        # Stack embeddings along new dimension (batch_size, 3, 512)\n",
        "        return tf.stack([anc, pos, neg], axis=1)\n",
        "\n",
        "encoder_model= create_encoder()\n",
        "sample_input = np.random.rand(1,128,128, 1)\n",
        "sample_output = encoder_model(sample_input)\n",
        "print(f\"DEMO OUTPUT: Input shape: {sample_input.shape}\")\n",
        "print(f\"Model output shape: {sample_output.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrT5Ieb9cUp6"
      },
      "outputs": [],
      "source": [
        "class TripletLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, margin=0.25):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        # Unpack from stacked tensor\n",
        "        anchor = y_pred[:, 0, :]  # (batch_size, 256)\n",
        "        positive = y_pred[:, 1, :]\n",
        "        negative = y_pred[:, 2, :]\n",
        "\n",
        "        pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
        "        neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
        "        loss = tf.maximum(pos_dist - neg_dist + self.margin, 0.0)\n",
        "        return tf.reduce_mean(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKDBXG5BI3E5"
      },
      "outputs": [],
      "source": [
        "def train_model():\n",
        "    anchors, positives, negatives = [], [], []\n",
        "\n",
        "    with tqdm(total=len(triplets), desc=\"Preprocessing\") as pbar:\n",
        "        for a, p, n in triplets:\n",
        "            anchors.append(pre_process(a))\n",
        "            positives.append(pre_process(p))\n",
        "            negatives.append(pre_process(n))\n",
        "            pbar.update(1)\n",
        "\n",
        "    anchors = np.array(anchors)\n",
        "    positives = np.array(positives)\n",
        "    negatives = np.array(negatives)\n",
        "\n",
        "    model = SiameseModel()\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss=TripletLoss())\n",
        "\n",
        "    # Train with proper input format\n",
        "    model.fit(\n",
        "        x=[anchors, positives, negatives],\n",
        "        y=np.zeros((len(triplets),)),  # Dummy labels\n",
        "        batch_size=32,\n",
        "        epochs=10\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdiGxTJGJ5Zy"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_spk, trials=50):\n",
        "    enrollments = {}\n",
        "    for speaker in test_spk:\n",
        "        if speaker in speaker_map:\n",
        "            enroll_file = random.choice(speaker_map[speaker])\n",
        "            enrollments[speaker] = pre_process(enroll_file)\n",
        "        else:\n",
        "            print(f\"Warning: Speaker {speaker} not found in speaker_map, skipping...\")\n",
        "            continue  # Skip to the next speaker\n",
        "\n",
        "    accurate = 0\n",
        "    for _ in range(trials):  # Use the trials argument for iterations\n",
        "        if random.random() < 0.5:\n",
        "            spk = random.choice(test_spk)\n",
        "            test_file = random.choice(speaker_map[spk])\n",
        "            enrollment = enrollments[spk]\n",
        "            true_label = 1\n",
        "        else:\n",
        "            spk1, spk2 = random.sample(test_spk, 2)\n",
        "            test_file = random.choice(speaker_map[spk1])\n",
        "            enrollment = enrollments[spk2]\n",
        "            true_label = 0\n",
        "\n",
        "        test_data = pre_process(test_file)\n",
        "\n",
        "        emb1 = model.encoder(enrollment[np.newaxis, :, :, np.newaxis])\n",
        "        emb2 = model.encoder(test_data[np.newaxis, :, :, np.newaxis])\n",
        "\n",
        "        emb1 = emb1.numpy()\n",
        "        emb2 = emb2.numpy()\n",
        "\n",
        "        # cosine similarity\n",
        "        similarity = np.dot(emb1, emb2.T) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
        "\n",
        "        if similarity > 0.5:\n",
        "            pred = 1\n",
        "        else:\n",
        "            pred = 0\n",
        "\n",
        "        if pred == true_label:\n",
        "            accurate += 1\n",
        "\n",
        "    accuracy = accurate / trials\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "id": "N4_Lp4JAL19O",
        "outputId": "a125d74f-47af-4b84-d884-1a25fcd52e7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total speakers in dataset: 22\n",
            "Speakers in test set: 5\n",
            "Example test speakers: ['id10724', 'id10393', 'id11089', 'id10943', 'id10956']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preprocessing: 100%|██████████| 500/500 [00:31<00:00, 15.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 5s/step - loss: 0.2501\n",
            "Epoch 2/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 6s/step - loss: 0.2482\n",
            "Epoch 3/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 5s/step - loss: 0.2458\n",
            "Epoch 4/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 5s/step - loss: 0.2407\n",
            "Epoch 5/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 6s/step - loss: 0.2417\n",
            "Epoch 6/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 5s/step - loss: 0.2387\n",
            "Epoch 7/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 5s/step - loss: 0.2374\n",
            "Epoch 8/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 5s/step - loss: 0.2226\n",
            "Epoch 9/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 5s/step - loss: 0.2200\n",
            "Epoch 10/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 5s/step - loss: 0.2069\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating triplets with hard negatives:   0%|          | 0/500 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed shape: (128, 128, 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Exception encountered when calling Conv2D.call().\n\n\u001b[1mNegative dimension size caused by subtracting 3 from 1 for '{{node siamese_model_11_1/sequential_15_1/conv2d_30_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](siamese_model_11_1/sequential_15_1/ExpandDims, siamese_model_11_1/sequential_15_1/conv2d_30_1/convolution/ReadVariableOp)' with input shapes: [128,128,1,1], [3,3,1,64].\u001b[0m\n\nArguments received by Conv2D.call():\n  • inputs=tf.Tensor(shape=(128, 128, 1, 1), dtype=float32)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-4e6605960b04>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtriplets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_triplets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_speaker_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_spk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"One shot verification accuracy: {accuracy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-f59177518244>\u001b[0m in \u001b[0;36mgenerate_triplets\u001b[0;34m(speakers, model, num_samples)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processed shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_anchor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0manchor_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_anchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Find the hardest negative (most similar to anchor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mhardest_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-a6bde939976c>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0manc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mneg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Conv2D.call().\n\n\u001b[1mNegative dimension size caused by subtracting 3 from 1 for '{{node siamese_model_11_1/sequential_15_1/conv2d_30_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](siamese_model_11_1/sequential_15_1/ExpandDims, siamese_model_11_1/sequential_15_1/conv2d_30_1/convolution/ReadVariableOp)' with input shapes: [128,128,1,1], [3,3,1,64].\u001b[0m\n\nArguments received by Conv2D.call():\n  • inputs=tf.Tensor(shape=(128, 128, 1, 1), dtype=float32)"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    all_spk = list(load_speaker_map(Data_dir).keys())\n",
        "    train_spk = random.sample(all_spk, int(0.8*len(all_spk)))\n",
        "    test_spk = [spk for spk in all_spk if spk not in train_spk]\n",
        "    print(f\"Total speakers in dataset: {len(speaker_map)}\")\n",
        "    print(f\"Speakers in test set: {len(test_spk)}\")\n",
        "    print(f\"Example test speakers: {test_spk[:5]}\")\n",
        "\n",
        "    model= train_model()\n",
        "    triplets = generate_triplets(load_speaker_map(Data_dir),model, num_samples)\n",
        "    accuracy = evaluate(model, test_spk,trials =50)\n",
        "    print(f\"One shot verification accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhA_qoEL3obW"
      },
      "source": [
        "0.42 accuracy without hard triplet mining\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiorxSuJ2gY5",
        "outputId": "95cea470-a281-4b10-c34f-046ba7280f73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One shot verification accuracy: 0.6\n"
          ]
        }
      ],
      "source": [
        "accuracy = evaluate(model, test_spk, trials =50)\n",
        "print(f\"One shot verification accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "neD4g1jppLLw",
        "outputId": "2a4a6201-3099-429e-ac3b-efa76acb3ab0"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'History' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-eed30e8da699>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'History' object is not subscriptable"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(model.history['loss'], label='Train Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzBNXXbU1aiA"
      },
      "outputs": [],
      "source": [
        "def test_custom_files(model, anchor_path, positive_path, negative_path, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Test custom audio files\n",
        "    Example usage:\n",
        "    test_custom_files(model,\n",
        "                    \"path/to/anchor.wav\",\n",
        "                    \"path/to/positive.wav\",\n",
        "                    \"path/to/negative.wav\")\n",
        "    \"\"\"\n",
        "    def process_and_embed(path):\n",
        "        spec = pre_process(path)\n",
        "        return model.encoder(np.expand_dims(spec, 0))\n",
        "\n",
        "    # Get embeddings\n",
        "    anchor_emb = process_and_embed(anchor_path)\n",
        "    positive_emb = process_and_embed(positive_path)\n",
        "    negative_emb = process_and_embed(negative_path)\n",
        "\n",
        "    # Calculate similarities\n",
        "    pos_sim = tf.reduce_sum(anchor_emb * positive_emb) / (\n",
        "        tf.norm(anchor_emb) * tf.norm(positive_emb)\n",
        "    )\n",
        "    neg_sim = tf.reduce_sum(anchor_emb * negative_emb) / (\n",
        "        tf.norm(anchor_emb) * tf.norm(negative_emb)\n",
        "    )\n",
        "\n",
        "    print(f\"\\nPositive Similarity: {pos_sim.numpy()[0]:.2%}\")\n",
        "    print(f\"Negative Similarity: {neg_sim.numpy()[0]:.2%}\")\n",
        "    print(f\"Verification Result: {'Accepted' if pos_sim > threshold else 'Rejected'}\")\n",
        "    print(f\"Imposter Result: {'Rejected' if neg_sim < threshold else 'Accepted'}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
